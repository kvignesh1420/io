{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elasticsearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Streaming structured data from Elasticsearch using Tensorflow-IO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/elasticsearch\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial focuses on streaming data from an [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html) cluster into a `tf.data.Dataset` which is then used in conjunction with `tf.keras` for training and inference.\n",
        "\n",
        "Elasticseach is primarily a distributed search engine which supports storing structured, unstructured along with geospatial and numeric data. For the purpose of this tutorial, a dataset with structured records is considered.\n",
        "\n",
        "**NOTE:** A basic understanding of [elasticsearch storage](https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html) will help you in following the tutorial with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required tensorflow-io and elasticsearch packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48B9eAMMhAgw"
      },
      "source": [
        "print(\"Installing the tensorflow-io-nightly package !\")\n",
        "!pip install tensorflow-io-nightly\n",
        "\n",
        "print(\"Installing the elasticsearch package !\")\n",
        "!pip install elasticsearch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from elasticsearch import Elasticsearch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgO11GTJaTj"
      },
      "source": [
        "### Validate tf and tfio imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX74RKfZ_TdF"
      },
      "source": [
        "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup the Elasticsearch instance\n",
        "\n",
        "For demo purposes, the open-source version of the elasticsearch package is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUj0878jPyz7"
      },
      "source": [
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
        "!shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 \n",
        "!tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Run the instance as a daemon process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ujlunrWgRx"
      },
      "source": [
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1))\n",
        "\n",
        "# Sleep for few seconds to let the instance start.\n",
        "time.sleep(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Once the instance has been started, grep for `elasticsearch` in the processes list to confirm the availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LqMJ1BEHm5"
      },
      "source": [
        "!ps -ef | grep elasticsearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBuRpiyf_kNS"
      },
      "source": [
        "query the base endpoint to retrieve information about the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILyohKWQ_XQS"
      },
      "source": [
        "!curl -X GET \"localhost:9200/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCy3zaCQJ7-"
      },
      "source": [
        "## PetFinder Dataset\n",
        "\n",
        "For the purpose of this tutorial, lets download the [PetFinder](https://www.kaggle.com/c/petfinder-adoption-prediction) dataset and feed the data into elasticsearch manually. The goal of this classification problem is predict if the pet will be adopted or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7"
      },
      "source": [
        "### Explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXyocIdKRSB"
      },
      "source": [
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "pf_df = pd.read_csv(csv_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-yt_c9u0sH"
      },
      "source": [
        "pf_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFL8nmnGVOc"
      },
      "source": [
        "For the purpose of the tutorial, modifications are made to the label column.\n",
        "0 will indicate the pet was not adopted, and 1 will indicate that it was.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Cg22bU0-na"
      },
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "pf_df['target'] = np.where(pf_df['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "pf_df = pf_df.drop(columns=['AdoptionSpeed', 'Description'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klnNOM5oGtH1"
      },
      "source": [
        "# Number of datapoints and columns\n",
        "len(pf_df), len(pf_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P"
      },
      "source": [
        "### Split the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ku_X0Wld59"
      },
      "source": [
        "train_df, test_df = train_test_split(pf_df, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL"
      },
      "source": [
        "### Store the train and test data in elasticsearch indices\n",
        "\n",
        "Storing the data in the local elasticsearch cluster simulates an environment for continuous remote data retrieval for training and inference purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwFImSqncLE"
      },
      "source": [
        "ES_NODES = \"http://localhost:9200\"\n",
        "\n",
        "def prepare_es_data(index, doc_type, df):\n",
        "  records = df.to_dict(orient=\"records\")\n",
        "  es_data = []\n",
        "  for idx, record in enumerate(records):\n",
        "    meta_dict = {\n",
        "          \"index\": {\n",
        "              \"_index\": index, \n",
        "              \"_type\": doc_type, \n",
        "              \"_id\": idx\n",
        "          }\n",
        "      }\n",
        "    es_data.append(meta_dict)\n",
        "    es_data.append(record)\n",
        "\n",
        "  return es_data\n",
        "\n",
        "def index_es_data(index, es_data):\n",
        "  es_client = Elasticsearch(hosts = [ES_NODES])\n",
        "  if es_client.indices.exists(index):\n",
        "      print(\"deleting the '{}' index.\".format(index))\n",
        "      res = es_client.indices.delete(index=index)\n",
        "      print(\"Response from server: {}\".format(res))\n",
        "\n",
        "  print(\"creating the '{}' index.\".format(index))\n",
        "  res = es_client.indices.create(index=index)\n",
        "  print(\"Response from server: {}\".format(res))\n",
        "\n",
        "  print(\"bulk index the data\")\n",
        "  res = es_client.bulk(index=index, body=es_data, refresh = True)\n",
        "  print(\"Errors: {}, Num of records indexed: {}\".format(res[\"errors\"], len(res[\"items\"])))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBiwCRBNGAu"
      },
      "source": [
        "train_es_data = prepare_es_data(index=\"train\", doc_type=\"pet\", df=train_df)\n",
        "test_es_data = prepare_es_data(index=\"test\", doc_type=\"pet\", df=test_df)\n",
        "\n",
        "index_es_data(index=\"train\", es_data=train_es_data)\n",
        "time.sleep(3)\n",
        "index_es_data(index=\"test\", es_data=test_es_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58q52py93jEf"
      },
      "source": [
        "### Define the tfio train dataset\n",
        "\n",
        "The `elasticsearch.ElasticsearchIODataset` class is utilized for streaming data from elasticsearch into tensorflow. The class inherits from `tf.data.Dataset` and thus exposes all the useful functionalities of `tf.data.Dataset` out of the box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHOcitbW2_d1"
      },
      "source": [
        "BATCH_SIZE=32\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "train_ds = tfio.experimental.elasticsearch.ElasticsearchIODataset(\n",
        "        nodes=[ES_NODES],\n",
        "        index=\"train\",\n",
        "        doc_type=\"pet\",\n",
        "        headers=HEADERS\n",
        "    )\n",
        "\n",
        "# Prepare a tuple of (data, label)\n",
        "train_ds = train_ds.map(lambda v: (v, v.pop(\"target\")))\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOD91R4BazMt"
      },
      "source": [
        "Fetch a sample batch and observe the `TensorSpec`'s of the columns/key's. This will help in defining the keras pre-processing layers for training the `tf.keras` model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7DzTScGa6lW"
      },
      "source": [
        "ds_iter = iter(train_ds)\n",
        "batch = next(ds_iter)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fAC5HDERL4-"
      },
      "source": [
        "### Define the keras preprocessing layers\n",
        "\n",
        "As per the [structured data tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers), it is recommended to use the [Keras Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) as they are more intuitive, and can be easily integrated with the models. However, the standard [feature_columns](https://www.tensorflow.org/api_docs/python/tf/feature_column) can also be used.\n",
        "\n",
        "For the sake of this tutorial we will follow the [structured data tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) and use numeric and categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzR7Li4SaQS"
      },
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for our feature.\n",
        "  normalizer = preprocessing.Normalization()\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer\n",
        "\n",
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = feature_ds.map(index)\n",
        "\n",
        "  # Learn the space of possible indices.\n",
        "  encoder.adapt(feature_ds)\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0Mmp_dT7yu"
      },
      "source": [
        "choosing the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0X9LEKoUfbU"
      },
      "source": [
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numeric features.\n",
        "for header in ['PhotoAmt', 'Fee']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "# Categorical features encoded as string.\n",
        "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x84lZJY164RI"
      },
      "source": [
        "## Build, compile and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHtpAMqLqmv"
      },
      "source": [
        "# Set the parameters\n",
        "\n",
        "OPTIMIZER=\"adam\"\n",
        "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "METRICS=['accuracy']\n",
        "EPOCHS=10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBmxxuj63jZ"
      },
      "source": [
        "# Convert the feature columns into a tf.keras layer\n",
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "\n",
        "# design/build the model\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(all_inputs, output)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTDFVxpSLfXI"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJMg-saLgeR"
      },
      "source": [
        "# fit the model\n",
        "model.fit(train_ds, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJW8za2qm4c"
      },
      "source": [
        "## Infer on the test data\n",
        "\n",
        "To infer on the test data, a different `ElasticsearchIODataset` is created which fetches data from the `test` index. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3FZOlSh2pmy"
      },
      "source": [
        "### Define the tfio test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjnM81lPROen"
      },
      "source": [
        "test_ds = tfio.experimental.elasticsearch.ElasticsearchIODataset(\n",
        "        nodes=[ES_NODES],\n",
        "        index=\"test\",\n",
        "        doc_type=\"pet\",\n",
        "        headers=HEADERS\n",
        "    )\n",
        "\n",
        "# Prepare a tuple of (data, label)\n",
        "test_ds = test_ds.map(lambda v: (v, v.pop(\"target\")))\n",
        "test_ds = test_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PapN5Q_241k"
      },
      "source": [
        "### evaluate the performance on the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hMtIe1X215P"
      },
      "source": [
        "res = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", res)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SvFjOJcdRyO"
      },
      "source": [
        "Note: Since the goal of this tutorial is to demonstrate Tensorflow-IO's capability to stream data from elasticsearch and train tf.keras models directly, improving the accuracy of the models is out of the tutorial's scope. However, the user can explore the dataset and play around with the feature columns to get a better model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QAS_3k1y3u"
      },
      "source": [
        "## References:\n",
        "\n",
        "- [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html)\n",
        "\n",
        "- [PetFinder Dataset](https://www.kaggle.com/c/petfinder-adoption-prediction)\n",
        "\n",
        "- [Classify Structured Data using Keras](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers#create_compile_and_train_the_model)\n"
      ]
    }
  ]
}