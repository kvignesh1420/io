{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elasticsearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow IO Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Streaming structured data from Elasticsearch using Tensorflow-IO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/io/tutorials/elasticsearch\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/io/blob/master/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/io/docs/tutorials/elasticsearch.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial focuses on streaming data from an [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html) cluster into a `tf.data.Dataset` which is then used in conjunction with `tf.keras` for training and inference.\n",
        "\n",
        "Elasticseach is primarily a distributed search engine which supports storing structured, unstructured, geospatial, numeric data etc. For the purpose of this tutorial, a dataset with structured records is utilized.\n",
        "\n",
        "**NOTE:** A basic understanding of [elasticsearch storage](https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html) will help you in following the tutorial with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup packages\n",
        "\n",
        "The `elasticsearch` package is utilized for preparing and storing the data within elasticsearch indices for demonstration purposes only. In real-world production clusters with numerous nodes, the cluster might be receiving the data from connectors like logstash etc.\n",
        "\n",
        "Once the data is available in the elasticsearch cluster, only `tensorflow-io` is required to stream the data into the models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required tensorflow-io and elasticsearch packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48B9eAMMhAgw",
        "outputId": "ec94e229-bade-4c59-cded-d252ca593cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "print(\"Installing the tensorflow-io-nightly package !\")\n",
        "!pip install tensorflow-io-nightly\n",
        "\n",
        "print(\"Installing the elasticsearch package !\")\n",
        "!pip install elasticsearch\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing the tensorflow-io-nightly package !\n",
            "Requirement already satisfied: tensorflow-io-nightly in /usr/local/lib/python3.6/dist-packages (0.15.0.dev20201011135342)\n",
            "Requirement already satisfied: tensorflow<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-io-nightly) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io-nightly) (3.1.0)\n",
            "Installing the elasticsearch package !\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.6/dist-packages (7.9.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from elasticsearch import Elasticsearch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgO11GTJaTj"
      },
      "source": [
        "### Validate tf and tfio imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX74RKfZ_TdF",
        "outputId": "d4f5f16a-b0ee-4d6b-af46-8abffd640514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow-io version: 0.15.0\n",
            "tensorflow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup the Elasticsearch instance\n",
        "\n",
        "For demo purposes, the open-source version of the elasticsearch package is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUj0878jPyz7",
        "outputId": "5ab3c5db-a153-4ac8-a978-3b64d543c37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
        "!shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 \n",
        "!tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2/\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 19:09:17--  https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
            "Resolving artifacts.elastic.co (artifacts.elastic.co)... 151.101.2.222, 151.101.66.222, 151.101.130.222, ...\n",
            "Connecting to artifacts.elastic.co (artifacts.elastic.co)|151.101.2.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 229941304 (219M) [application/x-gzip]\n",
            "Saving to: ‘elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.1’\n",
            "\n",
            "elasticsearch-oss-7 100%[===================>] 219.29M   141MB/s    in 1.6s    \n",
            "\n",
            "2020-10-12 19:09:19 (141 MB/s) - ‘elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.1’ saved [229941304/229941304]\n",
            "\n",
            "--2020-10-12 19:09:19--  https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
            "Resolving artifacts.elastic.co (artifacts.elastic.co)... 151.101.2.222, 151.101.66.222, 151.101.130.222, ...\n",
            "Connecting to artifacts.elastic.co (artifacts.elastic.co)|151.101.2.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173 [application/octet-stream]\n",
            "Saving to: ‘elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512.1’\n",
            "\n",
            "elasticsearch-oss-7 100%[===================>]     173  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-12 19:09:19 (8.98 MB/s) - ‘elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512.1’ saved [173/173]\n",
            "\n",
            "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Run the instance as a daemon process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ujlunrWgRx"
      },
      "source": [
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1))\n",
        "\n",
        "# Sleep for few seconds to let the instance start.\n",
        "time.sleep(15)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Once the instance has been started, grep for `elasticsearch` in the processes list to confirm the availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LqMJ1BEHm5",
        "outputId": "311e35cc-ba4f-4ae5-812f-d47135014d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!ps -ef | grep elasticsearch"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "daemon       501       1 11 19:06 ?        00:00:22 /content/elasticsearch-7.9.2/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms1g -Xmx1g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-3817198103791124653 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.9.2 -Des.path.conf=/content/elasticsearch-7.9.2/config -Des.distribution.flavor=oss -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.9.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n",
            "root        1053     786  0 19:09 ?        00:00:00 /bin/bash -c ps -ef | grep elasticsearch\n",
            "root        1055    1053  0 19:09 ?        00:00:00 grep elasticsearch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBuRpiyf_kNS"
      },
      "source": [
        "query the base endpoint to retrieve information about the cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILyohKWQ_XQS",
        "outputId": "5754d795-e057-486c-e760-a648414d0e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!curl -X GET \"localhost:9200/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\" : \"e598303c7c12\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"0jDTZWWKRMuOAT9Us2rARw\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.9.2\",\n",
            "    \"build_flavor\" : \"oss\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n",
            "    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.6.2\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CfKVmCvwcL7"
      },
      "source": [
        "### Explore the dataset\n",
        "\n",
        "For the purpose of this tutorial, lets download the [PetFinder](https://www.kaggle.com/c/petfinder-adoption-prediction) dataset and feed the data into elasticsearch manually. The goal of this classification problem is predict if the pet will be adopted or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXyocIdKRSB"
      },
      "source": [
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "pf_df = pd.read_csv(csv_file)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-yt_c9u0sH",
        "outputId": "c0df6522-cc5f-4c81-8030-03ab77084a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "pf_df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Fee</th>\n",
              "      <th>Description</th>\n",
              "      <th>PhotoAmt</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cat</td>\n",
              "      <td>3</td>\n",
              "      <td>Tabby</td>\n",
              "      <td>Male</td>\n",
              "      <td>Black</td>\n",
              "      <td>White</td>\n",
              "      <td>Small</td>\n",
              "      <td>Short</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>100</td>\n",
              "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cat</td>\n",
              "      <td>1</td>\n",
              "      <td>Domestic Medium Hair</td>\n",
              "      <td>Male</td>\n",
              "      <td>Black</td>\n",
              "      <td>Brown</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Not Sure</td>\n",
              "      <td>Not Sure</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>I just found it alone yesterday near my apartm...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dog</td>\n",
              "      <td>1</td>\n",
              "      <td>Mixed Breed</td>\n",
              "      <td>Male</td>\n",
              "      <td>Brown</td>\n",
              "      <td>White</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dog</td>\n",
              "      <td>4</td>\n",
              "      <td>Mixed Breed</td>\n",
              "      <td>Female</td>\n",
              "      <td>Black</td>\n",
              "      <td>Brown</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Short</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>150</td>\n",
              "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dog</td>\n",
              "      <td>1</td>\n",
              "      <td>Mixed Breed</td>\n",
              "      <td>Male</td>\n",
              "      <td>Black</td>\n",
              "      <td>No Color</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Short</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>This handsome yet cute boy is up for adoption....</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Type  Age  ... PhotoAmt AdoptionSpeed\n",
              "0  Cat    3  ...        1             2\n",
              "1  Cat    1  ...        2             0\n",
              "2  Dog    1  ...        7             3\n",
              "3  Dog    4  ...        8             2\n",
              "4  Dog    1  ...        3             2\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFL8nmnGVOc"
      },
      "source": [
        "For the purpose of the tutorial, modifications are made to the label column.\n",
        "0 will indicate the pet was not adopted, and 1 will indicate that it was.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Cg22bU0-na"
      },
      "source": [
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "pf_df['target'] = np.where(pf_df['AdoptionSpeed']==4, 0, 1)\n",
        "\n",
        "# Drop un-used columns.\n",
        "pf_df = pf_df.drop(columns=['AdoptionSpeed', 'Description'])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klnNOM5oGtH1",
        "outputId": "8c64d38c-6980-4734-f8bb-c20f86128201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Number of datapoints and columns\n",
        "len(pf_df), len(pf_df.columns)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11537, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF5K9xtmlT2P"
      },
      "source": [
        "### Split the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ku_X0Wld59",
        "outputId": "beeb0ca3-29d1-4edb-ed60-5f05ec393661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_df, test_df = train_test_split(pf_df, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  8075\n",
            "Number of testing sample:  3462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP5U4GqmhoL"
      },
      "source": [
        "### Store the train and test data in elasticsearch indices\n",
        "\n",
        "Storing the data in the local elasticsearch cluster simulates an environment for continuous remote data retrieval for training and inference purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwFImSqncLE"
      },
      "source": [
        "ES_NODES = \"http://localhost:9200\"\n",
        "\n",
        "def prepare_es_data(index, doc_type, df):\n",
        "  records = df.to_dict(orient=\"records\")\n",
        "  es_data = []\n",
        "  for idx, record in enumerate(records):\n",
        "    meta_dict = {\n",
        "          \"index\": {\n",
        "              \"_index\": index, \n",
        "              \"_type\": doc_type, \n",
        "              \"_id\": idx\n",
        "          }\n",
        "      }\n",
        "    es_data.append(meta_dict)\n",
        "    es_data.append(record)\n",
        "\n",
        "  return es_data\n",
        "\n",
        "def index_es_data(index, es_data):\n",
        "  es_client = Elasticsearch(hosts = [ES_NODES])\n",
        "  if es_client.indices.exists(index):\n",
        "      print(\"deleting the '{}' index.\".format(index))\n",
        "      res = es_client.indices.delete(index=index)\n",
        "      print(\"Response from server: {}\".format(res))\n",
        "\n",
        "  print(\"creating the '{}' index.\".format(index))\n",
        "  res = es_client.indices.create(index=index)\n",
        "  print(\"Response from server: {}\".format(res))\n",
        "\n",
        "  print(\"bulk index the data\")\n",
        "  res = es_client.bulk(index=index, body=es_data, refresh = True)\n",
        "  print(\"Errors: {}, Num of records indexed: {}\".format(res[\"errors\"], len(res[\"items\"])))\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBiwCRBNGAu",
        "outputId": "1bdbcb78-cd62-421b-de90-abab88879ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "train_es_data = prepare_es_data(index=\"train\", doc_type=\"pet\", df=train_df)\n",
        "test_es_data = prepare_es_data(index=\"test\", doc_type=\"pet\", df=test_df)\n",
        "\n",
        "index_es_data(index=\"train\", es_data=train_es_data)\n",
        "time.sleep(3)\n",
        "index_es_data(index=\"test\", es_data=test_es_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating the 'train' index.\n",
            "Response from server: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'train'}\n",
            "bulk index the data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
            "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Errors: False, Num of records indexed: 8075\n",
            "creating the 'test' index.\n",
            "Response from server: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'test'}\n",
            "bulk index the data\n",
            "Errors: False, Num of records indexed: 3462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mOrfOYrHpQj"
      },
      "source": [
        "## Prepare tfio datasets\n",
        "\n",
        "Once the data is available in the cluster, only `tensorflow-io` is required to stream the data from the indices. The `elasticsearch.ElasticsearchIODataset` class is utilized for this purpose. The class inherits from `tf.data.Dataset` and thus exposes all the useful functionalities of `tf.data.Dataset` out of the box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58q52py93jEf"
      },
      "source": [
        "### Training dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHOcitbW2_d1",
        "outputId": "6eb458e7-97ab-4e5f-e977-0c726e305459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE=32\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "train_ds = tfio.experimental.elasticsearch.ElasticsearchIODataset(\n",
        "        nodes=[ES_NODES],\n",
        "        index=\"train\",\n",
        "        doc_type=\"pet\",\n",
        "        headers=HEADERS\n",
        "    )\n",
        "\n",
        "# Prepare a tuple of (data, label)\n",
        "train_ds = train_ds.map(lambda v: (v, v.pop(\"target\")))\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connection successful: http://localhost:9200/_cluster/health\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me0zgeCQIsKH"
      },
      "source": [
        "### Testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-I9hUgIcXR",
        "outputId": "bfac22a8-ae6f-4499-cf53-14ca24a0c636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_ds = tfio.experimental.elasticsearch.ElasticsearchIODataset(\n",
        "        nodes=[ES_NODES],\n",
        "        index=\"test\",\n",
        "        doc_type=\"pet\",\n",
        "        headers=HEADERS\n",
        "    )\n",
        "\n",
        "# Prepare a tuple of (data, label)\n",
        "test_ds = test_ds.map(lambda v: (v, v.pop(\"target\")))\n",
        "test_ds = test_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connection successful: http://localhost:9200/_cluster/health\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOD91R4BazMt"
      },
      "source": [
        "Fetch a sample batch and observe the `TensorSpec`'s of the columns/key's. This will help in defining the keras pre-processing layers for training the `tf.keras` model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7DzTScGa6lW",
        "outputId": "92862a0c-4821-4336-eb22-b641c3cba09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ds_iter = iter(train_ds)\n",
        "batch = next(ds_iter)\n",
        "batch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Age': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
              "  array([ 48,   2,   3,   3,  32, 132,   4,   3,   2, 108,   2,   6,   4,\n",
              "           6,   2,  10,   6,   1,  12,  36,  36,   1,   3,  36,   7,  12,\n",
              "          39,   2,   3,   2,   2,   2])>,\n",
              "  'Breed1': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Golden Retriever', b'Domestic Short Hair', b'Terrier',\n",
              "         b'Domestic Medium Hair', b'Shih Tzu', b'Jack Russell Terrier',\n",
              "         b'Domestic Medium Hair', b'Domestic Short Hair', b'Mixed Breed',\n",
              "         b'Weimaraner', b'Mixed Breed', b'Domestic Short Hair',\n",
              "         b'Domestic Short Hair', b'Domestic Short Hair', b'Mixed Breed',\n",
              "         b'Doberman Pinscher', b'Domestic Medium Hair', b'Siamese',\n",
              "         b'Terrier', b'Mixed Breed', b'Pug', b'Mixed Breed', b'Mixed Breed',\n",
              "         b'Mixed Breed', b'Domestic Long Hair', b'Mixed Breed',\n",
              "         b'Mixed Breed', b'Persian', b'Mixed Breed', b'Mixed Breed',\n",
              "         b'Domestic Short Hair', b'Domestic Medium Hair'], dtype=object)>,\n",
              "  'Color1': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Golden', b'Yellow', b'Golden', b'Black', b'White', b'Brown',\n",
              "         b'Black', b'Black', b'White', b'Black', b'Brown', b'Yellow',\n",
              "         b'Brown', b'Golden', b'Brown', b'Black', b'Black', b'Brown',\n",
              "         b'Cream', b'Black', b'Brown', b'Black', b'Brown', b'Golden',\n",
              "         b'Brown', b'Cream', b'Black', b'Gray', b'Black', b'Black',\n",
              "         b'Brown', b'Black'], dtype=object)>,\n",
              "  'Color2': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'No Color', b'White', b'Cream', b'No Color', b'No Color',\n",
              "         b'White', b'White', b'Gray', b'No Color', b'No Color', b'Cream',\n",
              "         b'White', b'Cream', b'White', b'White', b'Brown', b'White',\n",
              "         b'Cream', b'No Color', b'No Color', b'White', b'Brown',\n",
              "         b'No Color', b'No Color', b'No Color', b'No Color', b'White',\n",
              "         b'No Color', b'No Color', b'No Color', b'White', b'Brown'],\n",
              "        dtype=object)>,\n",
              "  'Fee': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
              "  array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 150,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15,   0,\n",
              "           0,   0,   0,   0,   0,  50])>,\n",
              "  'FurLength': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Medium', b'Short', b'Short', b'Medium', b'Long', b'Medium',\n",
              "         b'Medium', b'Short', b'Short', b'Short', b'Short', b'Short',\n",
              "         b'Short', b'Short', b'Short', b'Short', b'Medium', b'Short',\n",
              "         b'Short', b'Short', b'Short', b'Short', b'Medium', b'Short',\n",
              "         b'Long', b'Short', b'Short', b'Long', b'Short', b'Short', b'Short',\n",
              "         b'Medium'], dtype=object)>,\n",
              "  'Gender': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Male', b'Male', b'Male', b'Male', b'Male', b'Female', b'Female',\n",
              "         b'Male', b'Male', b'Female', b'Male', b'Male', b'Male', b'Female',\n",
              "         b'Male', b'Male', b'Female', b'Male', b'Female', b'Female',\n",
              "         b'Male', b'Male', b'Male', b'Female', b'Female', b'Female',\n",
              "         b'Male', b'Female', b'Female', b'Female', b'Male', b'Female'],\n",
              "        dtype=object)>,\n",
              "  'Health': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Minor Injury', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy',\n",
              "         b'Healthy', b'Healthy'], dtype=object)>,\n",
              "  'MaturitySize': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Large', b'Medium', b'Medium', b'Small', b'Small', b'Small',\n",
              "         b'Medium', b'Medium', b'Medium', b'Medium', b'Medium', b'Medium',\n",
              "         b'Small', b'Medium', b'Small', b'Medium', b'Medium', b'Small',\n",
              "         b'Small', b'Large', b'Large', b'Medium', b'Medium', b'Medium',\n",
              "         b'Large', b'Small', b'Medium', b'Small', b'Medium', b'Medium',\n",
              "         b'Medium', b'Medium'], dtype=object)>,\n",
              "  'PhotoAmt': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
              "  array([3, 3, 2, 2, 4, 2, 1, 3, 3, 5, 6, 2, 5, 5, 2, 1, 5, 4, 5, 4, 1, 5,\n",
              "         3, 4, 2, 2, 5, 5, 2, 6, 3, 5])>,\n",
              "  'Sterilized': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'No', b'No', b'No', b'No', b'Not Sure', b'Yes', b'No', b'No',\n",
              "         b'Not Sure', b'No', b'No', b'No', b'No', b'Yes', b'No', b'No',\n",
              "         b'No', b'No', b'Not Sure', b'No', b'Yes', b'No', b'Not Sure',\n",
              "         b'Yes', b'No', b'No', b'No', b'Not Sure', b'No', b'No', b'No',\n",
              "         b'No'], dtype=object)>,\n",
              "  'Type': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Dog', b'Cat', b'Dog', b'Cat', b'Dog', b'Dog', b'Cat', b'Cat',\n",
              "         b'Dog', b'Dog', b'Dog', b'Cat', b'Cat', b'Cat', b'Dog', b'Dog',\n",
              "         b'Cat', b'Cat', b'Dog', b'Dog', b'Dog', b'Dog', b'Dog', b'Dog',\n",
              "         b'Cat', b'Dog', b'Dog', b'Cat', b'Dog', b'Dog', b'Cat', b'Cat'],\n",
              "        dtype=object)>,\n",
              "  'Vaccinated': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "  array([b'Yes', b'No', b'No', b'No', b'Not Sure', b'Yes', b'Yes', b'No',\n",
              "         b'No', b'Yes', b'No', b'No', b'Yes', b'No', b'Yes', b'No', b'Yes',\n",
              "         b'Yes', b'Not Sure', b'Yes', b'Yes', b'Not Sure', b'Not Sure',\n",
              "         b'Yes', b'No', b'Yes', b'Yes', b'Not Sure', b'No', b'No', b'No',\n",
              "         b'No'], dtype=object)>},\n",
              " <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
              " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 1, 0, 1, 1, 0, 1, 1, 1])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fAC5HDERL4-"
      },
      "source": [
        "### Define the keras preprocessing layers\n",
        "\n",
        "As per the [structured data tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers), it is recommended to use the [Keras Preprocessing Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) as they are more intuitive, and can be easily integrated with the models. However, the standard [feature_columns](https://www.tensorflow.org/api_docs/python/tf/feature_column) can also be used.\n",
        "\n",
        "For a better understanding of the `preprocessing_layers` in classifying structured data, please refer to the [structured data tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzR7Li4SaQS"
      },
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for our feature.\n",
        "  normalizer = preprocessing.Normalization()\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer\n",
        "\n",
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = feature_ds.map(index)\n",
        "\n",
        "  # Learn the space of possible indices.\n",
        "  encoder.adapt(feature_ds)\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so you can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0Mmp_dT7yu"
      },
      "source": [
        "Choose a subset of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0X9LEKoUfbU"
      },
      "source": [
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numeric features.\n",
        "for header in ['PhotoAmt', 'Fee']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "# Categorical features encoded as string.\n",
        "categorical_cols = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
        "                    'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Breed1']\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x84lZJY164RI"
      },
      "source": [
        "## Build, compile and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHtpAMqLqmv"
      },
      "source": [
        "# Set the parameters\n",
        "\n",
        "OPTIMIZER=\"adam\"\n",
        "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "METRICS=['accuracy']\n",
        "EPOCHS=10\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBmxxuj63jZ",
        "outputId": "93fcc174-408b-4789-b942-446d5c5a217d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert the feature columns into a tf.keras layer\n",
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "\n",
        "# design/build the model\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(all_inputs, output)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Type (InputLayer)               [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Color1 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Color2 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Gender (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "MaturitySize (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "FurLength (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Vaccinated (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Sterilized (InputLayer)         [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Health (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "PhotoAmt (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Fee (InputLayer)                [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup (StringLookup)    (None, 1)            0           Type[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_1 (StringLookup)  (None, 1)            0           Color1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_2 (StringLookup)  (None, 1)            0           Color2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_3 (StringLookup)  (None, 1)            0           Gender[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_4 (StringLookup)  (None, 1)            0           MaturitySize[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_5 (StringLookup)  (None, 1)            0           FurLength[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_6 (StringLookup)  (None, 1)            0           Vaccinated[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_7 (StringLookup)  (None, 1)            0           Sterilized[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_8 (StringLookup)  (None, 1)            0           Health[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_9 (StringLookup)  (None, 1)            0           Breed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "normalization (Normalization)   (None, 1)            3           PhotoAmt[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "normalization_1 (Normalization) (None, 1)            3           Fee[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding (CategoryEnco (None, 4)            0           string_lookup[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_1 (CategoryEn (None, 5)            0           string_lookup_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_2 (CategoryEn (None, 5)            0           string_lookup_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_3 (CategoryEn (None, 4)            0           string_lookup_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_4 (CategoryEn (None, 5)            0           string_lookup_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_5 (CategoryEn (None, 5)            0           string_lookup_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_6 (CategoryEn (None, 5)            0           string_lookup_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_7 (CategoryEn (None, 5)            0           string_lookup_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_8 (CategoryEn (None, 5)            0           string_lookup_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_9 (CategoryEn (None, 5)            0           string_lookup_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 50)           0           normalization[0][0]              \n",
            "                                                                 normalization_1[0][0]            \n",
            "                                                                 category_encoding[0][0]          \n",
            "                                                                 category_encoding_1[0][0]        \n",
            "                                                                 category_encoding_2[0][0]        \n",
            "                                                                 category_encoding_3[0][0]        \n",
            "                                                                 category_encoding_4[0][0]        \n",
            "                                                                 category_encoding_5[0][0]        \n",
            "                                                                 category_encoding_6[0][0]        \n",
            "                                                                 category_encoding_7[0][0]        \n",
            "                                                                 category_encoding_8[0][0]        \n",
            "                                                                 category_encoding_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           1632        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           2112        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            65          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,815\n",
            "Trainable params: 3,809\n",
            "Non-trainable params: 6\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTDFVxpSLfXI"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJMg-saLgeR",
        "outputId": "33d2536a-59f0-4024-e202-41e2ce2b2c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(train_ds, epochs=EPOCHS)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['Age'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "253/253 [==============================] - 4s 17ms/step - loss: 0.5913 - accuracy: 0.6472\n",
            "Epoch 2/10\n",
            "253/253 [==============================] - 4s 18ms/step - loss: 0.5637 - accuracy: 0.6967\n",
            "Epoch 3/10\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.5600 - accuracy: 0.7015\n",
            "Epoch 4/10\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.5498 - accuracy: 0.7206\n",
            "Epoch 5/10\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.5425 - accuracy: 0.7185\n",
            "Epoch 6/10\n",
            "253/253 [==============================] - 5s 18ms/step - loss: 0.5436 - accuracy: 0.7227\n",
            "Epoch 7/10\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.5410 - accuracy: 0.7263\n",
            "Epoch 8/10\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.5390 - accuracy: 0.7324\n",
            "Epoch 9/10\n",
            "253/253 [==============================] - 5s 19ms/step - loss: 0.5374 - accuracy: 0.7337\n",
            "Epoch 10/10\n",
            "253/253 [==============================] - 4s 17ms/step - loss: 0.5367 - accuracy: 0.7345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faeba44a128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJW8za2qm4c"
      },
      "source": [
        "## Infer on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hMtIe1X215P",
        "outputId": "4f348739-c968-43ca-ba41-9a458425554e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "res = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", res)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['Age'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 2s 17ms/step - loss: 0.5281 - accuracy: 0.7412\n",
            "test loss, test acc: [0.5280998945236206, 0.7411900758743286]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SvFjOJcdRyO"
      },
      "source": [
        "Note: Since the goal of this tutorial is to demonstrate Tensorflow-IO's capability to stream data from elasticsearch and train `tf.keras` models directly, improving the accuracy of the models is out of the current scope. However, the user can explore the dataset and play around with the feature columns and model architectures to get a better classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QAS_3k1y3u"
      },
      "source": [
        "## References:\n",
        "\n",
        "- [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html)\n",
        "\n",
        "- [PetFinder Dataset](https://www.kaggle.com/c/petfinder-adoption-prediction)\n",
        "\n",
        "- [Classify Structured Data using Keras](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers#create_compile_and_train_the_model)\n"
      ]
    }
  ]
}